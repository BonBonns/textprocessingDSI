<!-- Generated by pkgdown: do not edit by hand -->
<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>Quick Start • textprocessingDSI</title>

<!-- jquery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
<!-- Bootstrap -->

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha256-916EbMg70RQy9LHiGkXzG8hSg9EdNy97GazNG/aiY1w=" crossorigin="anonymous" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous"></script>

<!-- Font Awesome icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />

<!-- clipboard.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" integrity="sha256-FiZwavyI2V6+EXO1U+xzLG3IKldpiTFf3153ea9zikQ=" crossorigin="anonymous"></script>

<!-- sticky kit -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/sticky-kit/1.1.3/sticky-kit.min.js" integrity="sha256-c4Rlo1ZozqTPE2RLuvbusY3+SU1pQaJC0TjuhygMipw=" crossorigin="anonymous"></script>

<!-- pkgdown -->
<link href="pkgdown.css" rel="stylesheet">
<script src="pkgdown.js"></script>



<meta property="og:title" content="Quick Start" />



<!-- mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script>

<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->


  </head>

  <body>
    <div class="container template-title-body">
      <header>
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="index.html">textprocessingDSI</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">1.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="reference/index.html">Reference</a>
</li>
      </ul>
      
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
      
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      
      </header>

<div class="row">
  <div class="contents col-md-9">
    <div class="page-header">
      <h1>Quick Start</h1>
    </div>

<div id="quick-start" class="section level1">

<p>This page outlines the ways to processes a corpus once you have installed this package and its requirements. Two examples are demonstrated: an interactive method, and the point and click automated method. Both use the same underlying methods from this report, the first one just allows you to make decisions between the different parts of the pipeline as you will see in the example. For the two examples we will use the congressional globe debates corpus.</p>
<p>[TOC]</p>
<div id="interactive-pipeline" class="section level2">
<h2 class="hasAnchor">
<a href="#interactive-pipeline" class="anchor"></a>Interactive Pipeline</h2>
<p>This outlines the sequence of methods to use on a corpus to prepare it for topic modeling. This involves, formating, cleaning, filtering, and lemmatizing. The references for each function can be found on the reference page.</p>
<div id="formating-the-corpus" class="section level4">
<h4 class="hasAnchor">
<a href="#formating-the-corpus" class="anchor"></a>1. Formating the corpus</h4>
<p>The underlying assumption is that we can not load the entire corpus from a vector of texts in memory. Instead we will process the files directly. To enable this, a series of functions are provided in this package. In general to simplify distributing the computational aspects,to minimize the overhead of things like loading dictionaries, and to avoid dealing with corpora with unwieldy numbers of files, we want the corpus to be split into equal sized chunks (anywhere between 50 -1000). The easiest way to do this is to delimit the documents by newline and store many documents in each file. The number of documents per file is dependent on the size of each document.</p>
<p>In this example, we take the corpus, which is in a format where each document is in a different text file, and get it into the state described above.</p>
<div id="a--join-the-text-files" class="section level5">
<h5 class="hasAnchor">
<a href="#a--join-the-text-files" class="anchor"></a>a. Join the text files</h5>
<p>Here we use the <strong>rcpp_join ()</strong> method.</p>
<p><code>{r} files = rcpp_join("~/data/cd-globe/files/", "~/data/cd-globe/joined.dat", 1)</code></p>
<p>This will concatenate all the files in the files directory into a single file. Since we passed a ‘1’ as the final argument, the function will automatically replace all newlines within the files with spaces, and when joining files together will separate them with a newline. The return value (stored in files) is a character vector of all the filenames in that original directory. This way we can match documents with their filename down the pipeline.</p>
</div>
<div id="b--split-the-corpus-into-equal-sized-chunks" class="section level5">
<h5 class="hasAnchor">
<a href="#b--split-the-corpus-into-equal-sized-chunks" class="anchor"></a>b. split the corpus into equal sized chunks</h5>
<p>Here we split the large file we created above into several equal sized chunks. We do so with the <strong>rcpp_split()</strong> method.</p>
<p><code>{r} rcpp_split("~/data/cd-globe/joined.dat", "/data/cd-globe/split/", 'c', 50000)</code></p>
<p>This will split the “joined.dat” file into files with a size of 50MB (50,000 kilobytes). The files will be named in the format 00001.txt, 00002.txt …. This function preserves newlines and puts only as many lines would fit within each chunk. So you may see some files don’t reach exactly 50MB. Each file will maintain the format of one document per line.</p>
</div>
</div>
<div id="cleaning-and-lemmatizing-the-corpus" class="section level4">
<h4 class="hasAnchor">
<a href="#cleaning-and-lemmatizing-the-corpus" class="anchor"></a>2. Cleaning and Lemmatizing the corpus</h4>
<p>Now that the corpus is in a standardized format, we can call our cleaning function to tokenize and clean the corpus according to our specifications. The way that this package cleans a corpus is by cleaning each text file independently line by line. We do so by calling the <strong>clean_file()</strong> method. <strong>clean_file()</strong> is itself a wrapper around a python script that will handle reading the file in and performing the cleaning operations. We pass to that script all the arguments necessary to customize the cleaning according to our task. To see the list of arguments we can pass, look at the <strong>clean_file()</strong> documentation on the reference page. To clean a list of files we can call <strong>clean_corpus()</strong> which simply uses the <strong>parLapply</strong> function from the <strong>parallel</strong> package and the number of cores we want to use.</p>
<p><code>{r} clean_corpus("/data/cd-globe/split/", "/data/cd-globe/cleaned/", 30, "-lnprsd --maintain-newlines --min-size 2")</code></p>
<p>We can then pass our cleaned and tokenized corpus through a lemmatizer. This package uses the <strong>tree-tagger</strong> lemmatizer.</p>
<p><code>{r} lemma_corpus("/data/cd-globe/cleaned/", "/data/cd-globe/lemmatized/", 30)</code></p>
</div>
<div id="filtering-the-corpus" class="section level4">
<h4 class="hasAnchor">
<a href="#filtering-the-corpus" class="anchor"></a>3. Filtering the corpus</h4>
<p>For topic modeling it is useful to restrict the total number of unique words. This will limit the size of the topic terms matrix, speed up the time it takes to run the model, and reduce the number of meaningless words in our final model. When working with a large corpus, this step is crucial. The functions provided in this package allow the user to remove words based on the number of documents they appear in and the total number of times they appear across the corpus. To do so, we first need to create a data.table of the words, their frequency, and document frequency. We create this data.table using the <strong>summary_corpus()</strong> method.</p>
<div id="a--getting-the-word-frequencies" class="section level5">
<h5 class="hasAnchor">
<a href="#a--getting-the-word-frequencies" class="anchor"></a>a. getting the word frequencies</h5>
<p><code>{r} word_frequencies.dt = summary_corpus("/data/cd-globe/lemmatized/", 30) word_frequencies.dt</code></p>
<p><code>{r} ## Corpus consisting of 9 documents: ##  ##          Text Types Tokens Sentences ##           BNP  1125   3280        88 ##     Coalition   142    260         4 ##  Conservative   251    499        15 ##        Greens   322    679        21 ##        Labour   298    683        29 ##        LibDem   251    483        14 ##            PC    77    114         5 ##           SNP    88    134         4 ##          UKIP   346    723        27 ##  ## Source: /Users/smueller/Documents/GitHub/quanteda/vignettes/* on x86_64 by smueller ## Created: Tue Apr 16 17:14:22 2019 ## Notes:</code></p>
</div>
<div id="b--getting-the-list-of-words-we-want-to-remove" class="section level5">
<h5 class="hasAnchor">
<a href="#b--getting-the-list-of-words-we-want-to-remove" class="anchor"></a>b. getting the list of words we want to remove</h5>
<p><code>{r} sparse = get_sparse(word_frequencies.dt, length(files), .01) abundant = get_abundant(word_frequenices.dt, length(files), .95) terms = c(sparse,abundant)</code></p>
<p>This will give us a list of the terms that appeared in less than 1% of the documents, and the terms that appeared in 95% or more of the documents.</p>
</div>
<div id="c--filter-those-words-from-the-corpus" class="section level5">
<h5 class="hasAnchor">
<a href="#c--filter-those-words-from-the-corpus" class="anchor"></a>c. filter those words from the corpus</h5>
<p>Now we want to remove those words from the documents before we run the topic model.</p>
<p><code>{r} filter_corpus(terms, "/data/cd-globe/lemmatized/", 30)</code></p>
</div>
</div>
<div id="preparing-the-processed-corpus-for-topic-modeling" class="section level4">
<h4 class="hasAnchor">
<a href="#preparing-the-processed-corpus-for-topic-modeling" class="anchor"></a>4. Preparing the processed corpus for topic modeling</h4>
<p>The <strong>MALLET</strong> application expects the corpus to be in a form where the source data is a single file with each document instance per line. We can generate this with the <strong>rcpp_join() </strong> method.</p>
<p><code>{r} rcpp_join("/data/cd-globe/lemmatized", "/data/cd-globe/corpus.dat", 0)</code></p>
<p>Note the final argument of 0, which tells <strong>rcpp_join()</strong> to assume that each file is already delimiting documents by newline and nothing extra needs to be done.</p>
</div>
</div>
<div id="automated-pipeline" class="section level2">
<h2 class="hasAnchor">
<a href="#automated-pipeline" class="anchor"></a>Automated Pipeline</h2>
<p>This section outlines how to effectively perform the same steps as above on a corpus while avoiding much of the interaction. Note that using the automated method will limit your ability to interface with the pipeline. For example, you will not be able to change the thresholds for sparse and abundant terms based on the results you see. If it really doesn’t matter for your purposes, i.e you want to quickly prep a corpus for topic modeling without thinking too much use the method of this section.</p>
<p><code>{r} pipeline("/data/cd-globe/files/", "/data/cd-globe/output/", 30, "-lnprsd --maintain-newlines --min-size 2")</code></p>
<p>In the output directory you will find a file containing the fully processed corpus, a file with all the filenames, and a file with the parameters used in the pipeline.</p>
</div>
</div>

  </div>

</div>


      <footer>
      <div class="copyright">
  <p>Developed by Arthur Koehl.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.3.0.</p>
</div>
      </footer>
   </div>

  

  </body>
</html>

